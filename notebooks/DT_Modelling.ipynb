{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Environment & Dependencies\n",
    "\n",
    "Import all required libraries for data prep, modeling, and evaluation, plus RANDOM_STATE/TOPK_RATIO for reproducible baselines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score,\n",
    "    f1_score,\n",
    "    balanced_accuracy_score,\n",
    "    brier_score_loss,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import joblib\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TOPK_RATIO = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "Read the cleaned churn dataset and quickly inspect the head for sanity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('churn_clean.csv')\n",
    "assert DATA_PATH.exists(), f\"Data file not found: {DATA_PATH}\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Features & Target\n",
    "\n",
    "Drop the target column from features, store `Exited` as labels, and capture categorical/numeric column lists for preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Exited', axis=1)\n",
    "y = df['Exited']\n",
    "\n",
    "cat_cols = X.select_dtypes(include='object').columns.tolist()\n",
    "num_cols = X.select_dtypes(exclude='object').columns.tolist()\n",
    "cat_cols, num_cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Metric Helpers\n",
    "\n",
    "Top-K precision/recall plus an evaluation wrapper shared across models for consistent reporting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_precision_recall(y_true, y_proba, k_ratio: float = TOPK_RATIO):\n",
    "    \"\"\"Compute Precision@K and Recall@K.\"\"\"\n",
    "    assert 0 < k_ratio <= 1, 'k_ratio must be in (0, 1].'\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_proba = np.asarray(y_proba)\n",
    "\n",
    "    n = len(y_true)\n",
    "    k = max(1, int(np.floor(n * k_ratio)))\n",
    "    order = np.argsort(-y_proba)\n",
    "    topk_idx = order[:k]\n",
    "\n",
    "    y_topk = y_true[topk_idx]\n",
    "    tp_at_k = y_topk.sum()\n",
    "    total_pos = y_true.sum()\n",
    "\n",
    "    precision_at_k = tp_at_k / k\n",
    "    recall_at_k = tp_at_k / total_pos if total_pos > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        'K': k,\n",
    "        'Precision@K': precision_at_k,\n",
    "        'Recall@K': recall_at_k,\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_classifier(y_true, y_proba, threshold: float = 0.5, k_ratio: float = TOPK_RATIO):\n",
    "    \"\"\"Return ROC-AUC, PR-AUC, F1, Balanced Accuracy, Brier Score, Precision@K/Recall@K.\"\"\"\n",
    "    y_true_arr = y_true.values if hasattr(y_true, 'values') else np.asarray(y_true)\n",
    "    y_proba_arr = np.asarray(y_proba)\n",
    "    y_pred = (y_proba_arr >= threshold).astype(int)\n",
    "\n",
    "    metrics = {\n",
    "        'ROC-AUC': roc_auc_score(y_true_arr, y_proba_arr),\n",
    "        'PR-AUC': average_precision_score(y_true_arr, y_proba_arr),\n",
    "        'F1': f1_score(y_true_arr, y_pred),\n",
    "        'BalancedAccuracy': balanced_accuracy_score(y_true_arr, y_pred),\n",
    "        'BrierScore': brier_score_loss(y_true_arr, y_proba_arr),\n",
    "    }\n",
    "\n",
    "    topk = topk_precision_recall(y_true_arr, y_proba_arr, k_ratio=k_ratio)\n",
    "    metrics.update(topk)\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 60/20/20 Split\n",
    "\n",
    "Use a two-step stratified split to keep class balance consistent across train/validation/test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.4,\n",
    "    stratify=y,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp,\n",
    "    y_temp,\n",
    "    test_size=0.5,\n",
    "    stratify=y_temp,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "print('Train:', X_train.shape, 'Target ratio:', y_train.mean().round(3))\n",
    "print('Validation:', X_val.shape, 'Target ratio:', y_val.mean().round(3))\n",
    "print('Test:', X_test.shape, 'Target ratio:', y_test.mean().round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing & Decision Tree Pipeline\n",
    "\n",
    "One-hot encode categorical features, standardize numeric ones, then fit a class-balanced depth-limited tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "except TypeError:\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    ('categorical', ohe, cat_cols),\n",
    "    ('numeric', StandardScaler(), num_cols)\n",
    "])\n",
    "\n",
    "tree_params = dict(\n",
    "    max_depth=8,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    class_weight='balanced',\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "tree_pipeline = Pipeline([\n",
    "    ('preprocess', preprocess),\n",
    "    ('model', DecisionTreeClassifier(**tree_params))\n",
    "])\n",
    "tree_pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train The Decision Tree\n",
    "\n",
    "Fit the pipeline on the training split so we can reuse it for evaluation or deployment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_pipeline.fit(X_train, y_train)\n",
    "tree_pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Validation/Test Predictions\n",
    "\n",
    "Collect class predictions and probabilities for downstream metrics and plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "for dataset_name, (X_split, y_split) in {\n",
    "    'Validation': (X_val, y_val),\n",
    "    'Test': (X_test, y_test),\n",
    "}.items():\n",
    "    y_pred = tree_pipeline.predict(X_split)\n",
    "    y_proba = tree_pipeline.predict_proba(X_split)[:, 1]\n",
    "    predictions[dataset_name] = {\n",
    "        'y_true': y_split,\n",
    "        'y_pred': y_pred,\n",
    "        'y_proba': y_proba,\n",
    "    }\n",
    "\n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Aggregate Key Metrics\n",
    "\n",
    "Summarize ROC-AUC, PR-AUC, F1, Balanced Accuracy, Brier Score, and Top-K stats into a tidy DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_rows = []\n",
    "for dataset_name, values in predictions.items():\n",
    "    metrics = evaluate_classifier(values['y_true'], values['y_proba'], k_ratio=TOPK_RATIO)\n",
    "    metrics_rows.append({'Dataset': dataset_name, **metrics})\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_rows)\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Classification Reports & Confusion Matrices\n",
    "\n",
    "Print detailed reports and keep confusion matrices for visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrices = {}\n",
    "for dataset_name, values in predictions.items():\n",
    "    cm = confusion_matrix(values['y_true'], values['y_pred'])\n",
    "    conf_matrices[dataset_name] = cm\n",
    "    print(f\"Decision Tree - {dataset_name} classification report\")\n",
    "    print(classification_report(values['y_true'], values['y_pred']))\n",
    "\n",
    "conf_matrices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Figure Output Directory\n",
    "\n",
    "Ensure the shared figures folder exists before saving plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIG_DIR = Path('figures')\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Compute ROC/PR Curve Data\n",
    "\n",
    "Store FPR/TPR and Precision/Recall arrays for plotting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_data = {}\n",
    "for dataset_name, values in predictions.items():\n",
    "    fpr, tpr, _ = roc_curve(values['y_true'], values['y_proba'])\n",
    "    prec, rec, _ = precision_recall_curve(values['y_true'], values['y_proba'])\n",
    "    auc = roc_auc_score(values['y_true'], values['y_proba'])\n",
    "    ap = average_precision_score(values['y_true'], values['y_proba'])\n",
    "    curve_data[dataset_name] = {\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'auc': auc,\n",
    "        'ap': ap,\n",
    "    }\n",
    "curve_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Plot ROC Curves\n",
    "\n",
    "Overlay validation and test ROC curves to compare generalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "for dataset_name, roc_info in curve_data.items():\n",
    "    linestyle = '-' if dataset_name == 'Validation' else '--'\n",
    "    plt.plot(roc_info['fpr'], roc_info['tpr'], linestyle=linestyle, label=f\"{dataset_name} AUC = {roc_info['auc']:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve \u2014 Decision Tree Baseline')\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / 'roc_curve_tree_baseline.png', dpi=200)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Plot Precision-Recall Curves\n",
    "\n",
    "Visualize the recall\u2013precision trade-off for validation vs. test.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "for dataset_name, pr_info in curve_data.items():\n",
    "    linestyle = '-' if dataset_name == 'Validation' else '--'\n",
    "    plt.plot(pr_info['recall'], pr_info['precision'], linestyle=linestyle, label=f\"{dataset_name} AP = {pr_info['ap']:.3f}\")\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision\u2013Recall Curve \u2014 Decision Tree Baseline')\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / 'pr_curve_tree_baseline.png', dpi=200)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Plot Confusion Matrices\n",
    "\n",
    "Heatmaps of validation and test confusion matrices for quick error inspection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(9, 4))\n",
    "for ax, dataset_name in zip(axes, ['Validation', 'Test']):\n",
    "    sns.heatmap(conf_matrices[dataset_name], annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax)\n",
    "    ax.set_title(f'{dataset_name} Confusion Matrix')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / 'confusion_matrices_tree_baseline.png', dpi=200)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Feature Importances\n",
    "\n",
    "Review the top features driving the depth-limited tree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tree_pipeline.named_steps['model']\n",
    "feature_names = tree_pipeline.named_steps['preprocess'].get_feature_names_out()\n",
    "importances = pd.Series(model.feature_importances_, index=feature_names)\n",
    "top_features = importances.sort_values(ascending=False).head(20)\n",
    "top_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=top_features.values, y=top_features.index, color='tab:olive')\n",
    "plt.xlabel('Feature importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Top 20 Features \u2014 Decision Tree Baseline')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Export Metrics & Predictions\n",
    "\n",
    "Persist the summary metrics and per-sample probabilities for downstream comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = Path('reports')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "metrics_df.to_csv(OUTPUT_DIR / 'decision_tree_metrics_en.csv', index=False)\n",
    "\n",
    "pred_rows = []\n",
    "for dataset_name, values in predictions.items():\n",
    "    pred_rows.append(pd.DataFrame({\n",
    "        'dataset': dataset_name,\n",
    "        'y_true': values['y_true'].values,\n",
    "        'y_proba': values['y_proba'],\n",
    "    }))\n",
    "pred_df = pd.concat(pred_rows, ignore_index=True)\n",
    "pred_df.to_csv(OUTPUT_DIR / 'decision_tree_predictions_en.csv', index=False)\n",
    "\n",
    "print('Saved metrics and predictions to', OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Persist The Trained Pipeline\n",
    "\n",
    "Save the full tree pipeline as joblib so it can be reloaded without retraining.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = Path('models')\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_PATH = MODEL_DIR / 'decision_tree_baseline.joblib'\n",
    "joblib.dump(tree_pipeline, MODEL_PATH)\n",
    "MODEL_PATH\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}