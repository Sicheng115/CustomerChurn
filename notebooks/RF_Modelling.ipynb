{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Environment & Dependencies\n",
    "\n",
    "Import everything needed for data prep, modeling, plotting, and evaluation plus constants for reproducible splits and Top-K metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score,\n",
    "    f1_score,\n",
    "    balanced_accuracy_score,\n",
    "    brier_score_loss,\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import joblib\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "TOPK_RATIO = 0.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data\n",
    "\n",
    "Bring the cleaned churn dataset into memory and inspect the head to confirm schema.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('churn_clean.csv')\n",
    "assert DATA_PATH.exists(), f\"Data file not found: {DATA_PATH}\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Features & Target\n",
    "\n",
    "Separate the `Exited` label from features and capture categorical/numeric column names for preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Exited', axis=1)\n",
    "y = df['Exited']\n",
    "\n",
    "cat_cols = X.select_dtypes(include='object').columns.tolist()\n",
    "num_cols = X.select_dtypes(exclude='object').columns.tolist()\n",
    "cat_cols, num_cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Metric Helpers\n",
    "\n",
    "Reusable helper functions for Precision@K/Recall@K and a consolidated evaluation dict.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_precision_recall(y_true, y_proba, k_ratio: float = TOPK_RATIO):\n",
    "    \"\"\"Compute Precision@K and Recall@K.\"\"\"\n",
    "    assert 0 < k_ratio <= 1, 'k_ratio must be in (0, 1].'\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_proba = np.asarray(y_proba)\n",
    "\n",
    "    n = len(y_true)\n",
    "    k = max(1, int(np.floor(n * k_ratio)))\n",
    "    order = np.argsort(-y_proba)\n",
    "    topk_idx = order[:k]\n",
    "\n",
    "    y_topk = y_true[topk_idx]\n",
    "    tp_at_k = y_topk.sum()\n",
    "    total_pos = y_true.sum()\n",
    "\n",
    "    precision_at_k = tp_at_k / k\n",
    "    recall_at_k = tp_at_k / total_pos if total_pos > 0 else 0.0\n",
    "\n",
    "    return {\n",
    "        'K': k,\n",
    "        'Precision@K': precision_at_k,\n",
    "        'Recall@K': recall_at_k,\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_classifier(y_true, y_proba, threshold: float = 0.5, k_ratio: float = TOPK_RATIO):\n",
    "    \"\"\"Return ROC-AUC, PR-AUC, F1, Balanced Accuracy, Brier Score, Precision@K/Recall@K.\"\"\"\n",
    "    y_true_arr = y_true.values if hasattr(y_true, 'values') else np.asarray(y_true)\n",
    "    y_proba_arr = np.asarray(y_proba)\n",
    "    y_pred = (y_proba_arr >= threshold).astype(int)\n",
    "\n",
    "    metrics = {\n",
    "        'ROC-AUC': roc_auc_score(y_true_arr, y_proba_arr),\n",
    "        'PR-AUC': average_precision_score(y_true_arr, y_proba_arr),\n",
    "        'F1': f1_score(y_true_arr, y_pred),\n",
    "        'BalancedAccuracy': balanced_accuracy_score(y_true_arr, y_pred),\n",
    "        'BrierScore': brier_score_loss(y_true_arr, y_proba_arr),\n",
    "    }\n",
    "\n",
    "    topk = topk_precision_recall(y_true_arr, y_proba_arr, k_ratio=k_ratio)\n",
    "    metrics.update(topk)\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 60/20/20 Split\n",
    "\n",
    "Perform stratified train/validation/test splits (60/20/20) to maintain class balance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.4,\n",
    "    stratify=y,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp,\n",
    "    y_temp,\n",
    "    test_size=0.5,\n",
    "    stratify=y_temp,\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "\n",
    "print('Train:', X_train.shape, 'Target ratio:', y_train.mean().round(3))\n",
    "print('Validation:', X_val.shape, 'Target ratio:', y_val.mean().round(3))\n",
    "print('Test:', X_test.shape, 'Target ratio:', y_test.mean().round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Preprocessing & Random Forest Pipeline\n",
    "\n",
    "One-hot encode categoricals, scale numeric features, and attach a class-weighted, depth-limited random forest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "except TypeError:\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "\n",
    "preprocess = ColumnTransformer([\n",
    "    ('categorical', ohe, cat_cols),\n",
    "    ('numeric', StandardScaler(), num_cols)\n",
    "])\n",
    "\n",
    "rf_params = dict(\n",
    "    n_estimators=500,\n",
    "    max_depth=12,\n",
    "    min_samples_split=4,\n",
    "    min_samples_leaf=4,\n",
    "    class_weight='balanced_subsample',\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "rf_pipeline = Pipeline([\n",
    "    ('preprocess', preprocess),\n",
    "    ('model', RandomForestClassifier(**rf_params))\n",
    "])\n",
    "rf_pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train The Random Forest\n",
    "\n",
    "Fit the pipeline on the training split so preprocessing and the ensemble stay coupled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipeline.fit(X_train, y_train)\n",
    "rf_pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Generate Validation/Test Predictions\n",
    "\n",
    "Store predictions and probabilities for validation/test splits to drive metrics and plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = {}\n",
    "for dataset_name, (X_split, y_split) in {\n",
    "    'Validation': (X_val, y_val),\n",
    "    'Test': (X_test, y_test),\n",
    "}.items():\n",
    "    y_pred = rf_pipeline.predict(X_split)\n",
    "    y_proba = rf_pipeline.predict_proba(X_split)[:, 1]\n",
    "    predictions[dataset_name] = {\n",
    "        'y_true': y_split,\n",
    "        'y_pred': y_pred,\n",
    "        'y_proba': y_proba,\n",
    "    }\n",
    "\n",
    "predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Aggregate Key Metrics\n",
    "\n",
    "Summarize ROC-AUC, PR-AUC, F1, Balanced Accuracy, Brier Score, and Top-K metrics per split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_rows = []\n",
    "for dataset_name, values in predictions.items():\n",
    "    metrics = evaluate_classifier(values['y_true'], values['y_proba'], k_ratio=TOPK_RATIO)\n",
    "    metrics_rows.append({'Dataset': dataset_name, **metrics})\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_rows)\n",
    "metrics_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Classification Reports & Confusion Matrices\n",
    "\n",
    "Print detailed classification reports and keep confusion matrices for visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrices = {}\n",
    "for dataset_name, values in predictions.items():\n",
    "    cm = confusion_matrix(values['y_true'], values['y_pred'])\n",
    "    conf_matrices[dataset_name] = cm\n",
    "    print(f\"Random Forest - {dataset_name} classification report\")\n",
    "    print(classification_report(values['y_true'], values['y_pred']))\n",
    "\n",
    "conf_matrices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Figure Output Directory\n",
    "\n",
    "Ensure the figure directory exists before saving plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FIG_DIR = Path('figures')\n",
    "FIG_DIR.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Compute ROC/PR Curve Data\n",
    "\n",
    "Capture FPR/TPR and Precision/Recall arrays plus AUC/AP values for both splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve_data = {}\n",
    "for dataset_name, values in predictions.items():\n",
    "    fpr, tpr, _ = roc_curve(values['y_true'], values['y_proba'])\n",
    "    prec, rec, _ = precision_recall_curve(values['y_true'], values['y_proba'])\n",
    "    auc = roc_auc_score(values['y_true'], values['y_proba'])\n",
    "    ap = average_precision_score(values['y_true'], values['y_proba'])\n",
    "    curve_data[dataset_name] = {\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'auc': auc,\n",
    "        'ap': ap,\n",
    "    }\n",
    "curve_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Plot ROC Curves\n",
    "\n",
    "Overlay validation/test ROC curves to compare ensemble performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "for dataset_name, roc_info in curve_data.items():\n",
    "    linestyle = '-' if dataset_name == 'Validation' else '--'\n",
    "    plt.plot(roc_info['fpr'], roc_info['tpr'], linestyle=linestyle, label=f\"{dataset_name} AUC = {roc_info['auc']:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=1)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve \u2014 Random Forest Baseline')\n",
    "plt.legend(loc='lower right')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / 'roc_curve_rf_baseline.png', dpi=200)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Plot Precision-Recall Curves\n",
    "\n",
    "Visualize the recall\u2013precision trade-off for each split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "for dataset_name, pr_info in curve_data.items():\n",
    "    linestyle = '-' if dataset_name == 'Validation' else '--'\n",
    "    plt.plot(pr_info['recall'], pr_info['precision'], linestyle=linestyle, label=f\"{dataset_name} AP = {pr_info['ap']:.3f}\")\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision\u2013Recall Curve \u2014 Random Forest Baseline')\n",
    "plt.legend(loc='lower left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / 'pr_curve_rf_baseline.png', dpi=200)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Plot Confusion Matrices\n",
    "\n",
    "Heatmaps for validation and test confusion matrices offer a quick error view.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(9, 4))\n",
    "for ax, dataset_name in zip(axes, ['Validation', 'Test']):\n",
    "    sns.heatmap(conf_matrices[dataset_name], annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax)\n",
    "    ax.set_title(f'{dataset_name} Confusion Matrix')\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Actual')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIG_DIR / 'confusion_matrices_rf_baseline.png', dpi=200)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Feature Importances\n",
    "\n",
    "Inspect which engineered features drive the random forest decisions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = rf_pipeline.named_steps['preprocess'].get_feature_names_out()\n",
    "importances = pd.Series(rf_pipeline.named_steps['model'].feature_importances_, index=feature_names)\n",
    "top_features = importances.sort_values(ascending=False).head(20)\n",
    "top_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "sns.barplot(x=top_features.values, y=top_features.index, color='tab:green')\n",
    "plt.xlabel('Feature importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Top 20 Features \u2014 Random Forest Baseline')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Export Metrics & Predictions\n",
    "\n",
    "Persist the summary metrics and raw prediction probabilities for downstream comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = Path('reports')\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "metrics_df.to_csv(OUTPUT_DIR / 'random_forest_metrics_en.csv', index=False)\n",
    "\n",
    "pred_rows = []\n",
    "for dataset_name, values in predictions.items():\n",
    "    pred_rows.append(pd.DataFrame({\n",
    "        'dataset': dataset_name,\n",
    "        'y_true': values['y_true'].values,\n",
    "        'y_proba': values['y_proba'],\n",
    "    }))\n",
    "pred_df = pd.concat(pred_rows, ignore_index=True)\n",
    "pred_df.to_csv(OUTPUT_DIR / 'random_forest_predictions_en.csv', index=False)\n",
    "\n",
    "print('Saved metrics and predictions to', OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Persist The Trained Pipeline\n",
    "\n",
    "Save the fitted RF pipeline (preprocessing + estimator) as a joblib artifact for reuse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIR = Path('models')\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_PATH = MODEL_DIR / 'random_forest_baseline.joblib'\n",
    "joblib.dump(rf_pipeline, MODEL_PATH)\n",
    "MODEL_PATH\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}